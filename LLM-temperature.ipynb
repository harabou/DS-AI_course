{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPndNKDVqcBePQLvSDLVZCt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# LLMの出力におけるtemperatureの役割の確認\n","ELYZA社の大規模言語モデルLlama-3-ELYZA-JP-8Bを使用し、temperatureの値がLLMの出力結果にどのように影響するかを確認する。"],"metadata":{"id":"UFreScJZGif-"}},{"cell_type":"markdown","source":["必要なライブラリをインストールし、言語モデルをダウンロドする。\n","\n","5分程度の時間を要する。"],"metadata":{"id":"GudwxNZVbicU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwcF17YBeipu"},"outputs":[],"source":["!pip install llama-cpp-python\n","\n","!wget https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B-GGUF/resolve/main/Llama-3-ELYZA-JP-8B-q4_k_m.gguf"]},{"cell_type":"markdown","source":["ダウンロードした言語モデルを読み込む"],"metadata":{"id":"eMbv-PXWbse5"}},{"cell_type":"code","source":["from llama_cpp import Llama\n","\n","llm = Llama(model_path=\"Llama-3-ELYZA-JP-8B-q4_k_m.gguf\")"],"metadata":{"id":"TE69_laTeyCS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## temperatureパラメータの値によるLLMの回答の違いの確認\n","temperatureの値をいろいろな値に変えて、LLMからの回答がどのように変化するか確認しなさい。"],"metadata":{"id":"SgYcXzEMcVa8"}},{"cell_type":"code","source":["# LLMに入力するプロンプトの文字列\n","prompt = \"吾輩は\"\n","\n","# LLMによるプロンプトに対する出力の推定\n","output = llm(prompt, temperature=0.1, stop=\"\\n\", max_tokens=50, echo=False)\n","\n","# LLMから出力された文字列を画面に表示\n","print()\n","print(output[\"choices\"][0][\"text\"])"],"metadata":{"id":"9ZwyClr9TGR7"},"execution_count":null,"outputs":[]}]}